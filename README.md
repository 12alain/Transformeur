

1. [Transformeurs](#Transformeurs)

    1.1 [La Fonction pipeline ](#La-fonction-pipeline)

    1.2 [Comparaison avec les mod√®les pr√©c√©dents (RNN, LSTM)](#comparaison-avec-les-mod√®les-pr√©c√©dents-rnn-lstm)

2. [Architecture des Transformeurs](#architecture-des-transformeurs)

    2.1 [Input](#input)

    2.2 [Tokenisation](#tokenisation)

    2.3 [Embeddings](#embeddings)

    2.4 [Position encoding](#position-encoding)

    2.5 [Self-attention](#self-attention)

![Logo](https://www.researchgate.net/publication/323904682/figure/fig1/AS:606458626465792@1521602412057/The-Transformer-model-architecture.png)




# Transformeurs

Les transformeurs sont une architecture de r√©seau neuronal utilis√©e principalement dans le traitement du langage naturel (NLP) et dans les t√¢ches de traduction automatique. Ils ont √©t√© introduits dans le papier "Attention is All You Need" par Vaswani et al. en 2017. 


# La fonction pipeline

L'objet le plus fondamental dans la biblioth√®que ü§ó Transformers est la fonction `pipeline()`. Elle connecte un mod√®le avec ses √©tapes de pr√©traitement et de post-traitement n√©cessaires, nous permettant d'entrer directement n'importe quel texte et d'obtenir une r√©ponse intelligible.

```javascript
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier("I've been waiting for a HuggingFace course my whole life.")
```

```javascript
[{'label': 'POSITIVE', 'score': 0.9598047137260437}]
```



# Comparaison avec les mod√®les pr√©c√©dents (RNN, LSTM)


| Caract√©ristique                                | Transformeurs                                 | RNN                                       | LSTM                                      |
|------------------------------------------------|-----------------------------------------------|-------------------------------------------|-------------------------------------------|
| **Architecture**                               | Bas√© sur des m√©canismes d'attention           | R√©seau de neurones r√©current              | RNN avec des cellules de m√©moire          |
| **Parall√©lisme**                               | Oui (traitement parall√®le des s√©quences)      | Non (traitement s√©quentiel)               | Non (traitement s√©quentiel)               |
| **Complexit√© de calcul**                       | Moins efficace pour les s√©quences tr√®s longues | Efficace pour les s√©quences courtes       | Efficace pour les s√©quences moyennes      |
| **Performance**                                | State-of-the-art sur de nombreuses t√¢ches NLP | Bon pour les t√¢ches s√©quentielles simples | Bon pour les t√¢ches s√©quentielles complexes |
| **Entra√Ænement**                               | N√©cessite plus de ressources (m√©moire et calcul) | Moins co√ªteux                             | Co√ªt mod√©r√©                               |
| **Applications typiques**                      | Traduction, g√©n√©ration de texte, r√©sum√©, etc. | Pr√©vision de s√©ries temporelles, g√©n√©ration de s√©quences simples | Pr√©vision de s√©ries temporelles complexes, g√©n√©ration de s√©quences plus complexes |

