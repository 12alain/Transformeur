

1. [Transformeurs](#Transformeurs)

    1.1 [La Fonction pipeline ](#La-fonction-pipeline)

    1.2 [Le Transfert Learning](#Le-Transfert-learning)

    1.3 [Comparaison avec les mod√®les pr√©c√©dents (RNN, LSTM)](#comparaison-avec-les-mod√®les-pr√©c√©dents-rnn-lstm)

2. [Architecture des Transformeurs](#architecture-des-transformeurs)

    2.3 [Tokenisation](#tokenisation)
    
    2.2 [vocabulaire](#vocabulaire)

    2.3 [Embeddings](#embeddings)

    2.4 [Encodage de position](#encodage-de-position)

    2.5 [Mecanisme d'attention](#mecanisme-dattention)

    2.6 [Feed Forward](#feed-forward)
    
    2.7 [Encodeur](#encodeur)

    2.8 [Decodeur](#decodeur)
  


![Logo](images/The-Transformer-model-architecture.png)



# Transformeurs

Les transformeurs sont une architecture de r√©seau neuronal utilis√©e principalement dans le traitement du langage naturel (NLP) et dans les t√¢ches de traduction automatique. Ils ont √©t√© introduits dans le papier "Attention is All You Need" par Vaswani et al. en 2017. 


# La fonction pipeline

L'objet le plus fondamental dans la biblioth√®que ü§ó Transformers est la fonction `pipeline()`. Elle connecte un mod√®le avec ses √©tapes de pr√©traitement et de post-traitement n√©cessaires, nous permettant d'entrer directement n'importe quel texte et d'obtenir une r√©ponse intelligible.

```javascript
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier("I've been waiting for a HuggingFace course my whole life.")
```

```javascript
[{'label': 'POSITIVE', 'score': 0.9598047137260437}]
```
On peut m√™me passer plusieurs phrases !


```javascript
classifier(
    ["I've been waiting for a HuggingFace course my whole life.", "I hate this so much!"]
)
```

Par d√©faut, ce pipeline s√©lectionne un mod√®le pr√©-entra√Æn√© particulier qui a √©t√© affin√© pour l'analyse des sentiments en anglais. Le mod√®le est t√©l√©charg√© et mis en cache lorsque vous cr√©ez l' classifierobjet. Si vous r√©ex√©cutez la commande, le mod√®le mis en cache sera utilis√© √† la place et il n'est pas n√©cessaire de t√©l√©charger √† nouveau le mod√®le.

Trois √©tapes principales sont impliqu√©es lorsque vous transmettez du texte √† un pipeline :

- Le texte est pr√©trait√© dans un format que le mod√®le peut comprendre.

- Les entr√©es pr√©trait√©es sont transmises au mod√®le.

- Les pr√©dictions du mod√®le sont post-trait√©es afin que vous puissiez les comprendre.

voici une liste de certains  des pipelines actuellement disponibles que vous pouvez tester en lisant la documentation:

- fill-mask
- ner(reconnaissance d'entit√© nomm√©e)
- question-answering
- sentiment-analysis

# Le Transfert learning

Le transfert learning (ou apprentissage par transfert) est une technique en apprentissage automatique et en intelligence artificielle o√π un mod√®le form√© sur une t√¢che est r√©utilis√© comme point de d√©part pour un mod√®le sur une autre t√¢che. Cela est particuli√®rement utile lorsque les donn√©es disponibles pour la nouvelle t√¢che sont limit√©es.

Par exemple : 

Nous souhaitons d√©velopper un mod√®le pour classifier des images de radiographies afin de d√©tecter des maladies telles que la tuberculose, la pneumonie et le COVID-19. Pour ce faire, nous pouvons utiliser le transfert learning sur le mod√®le VGG16 pr√©-entra√Æn√© sur ImageNet.

# Comparaison avec les mod√®les pr√©c√©dents (RNN, LSTM)

| Caract√©ristique                                | Transformeurs                                 | RNN                                       | LSTM                                      |
|------------------------------------------------|-----------------------------------------------|-------------------------------------------|-------------------------------------------|
| **Architecture**                               | Bas√© sur des m√©canismes d'attention           | R√©seau de neurones r√©current              | RNN avec des cellules de m√©moire          |
| **Parall√©lisme**                               | Oui (traitement parall√®le des s√©quences)      | Non (traitement s√©quentiel)               | Non (traitement s√©quentiel)               |
| **Captation de contexte √† longue distance**    | Tr√®s efficace (gr√¢ce √† l'attention)           | Moins efficace                            | Efficace (gr√¢ce aux cellules de m√©moire)  |
| **Complexit√© de calcul**                       | Moins efficace pour les s√©quences tr√®s longues | Efficace pour les s√©quences courtes       | Efficace pour les s√©quences moyennes      |
| **Performance**                                | State-of-the-art sur de nombreuses t√¢ches NLP | Bon pour les t√¢ches s√©quentielles simples | Bon pour les t√¢ches s√©quentielles complexes |
| **Entra√Ænement**                               | N√©cessite plus de ressources (m√©moire et calcul) | Moins co√ªteux                             | Co√ªt mod√©r√©                               |
| **Applications typiques**                      | Traduction, g√©n√©ration de texte, r√©sum√©, etc. | Pr√©vision de s√©ries temporelles, g√©n√©ration de s√©quences simples | Pr√©vision de s√©ries temporelles complexes, g√©n√©ration de s√©quences plus complexes |
| **Gestion des d√©pendances √† longue port√©e**    | Tr√®s bonne                                   | Faible                                    | Bonne                                     |
| **Besoin en m√©moire**                          | Elev√© (d√©pend de la longueur de la s√©quence)  | Moins √©lev√©                               | Mod√©r√©                                    |


# Tokenisation

La tokenisation est le processus de division d'un texte en unit√©s plus petites appel√©es "tokens". Ces tokens peuvent √™tre des mots, des sous-mots, des caract√®res ou des phrases.

```javascript
Texte: je suis content

Tokens :["je","suis","content"]
```
Dans cet exemple , nous avons discut√© des bases de la tokenisation, mais il existe en r√©alit√© plusieurs m√©thodes et techniques avanc√©es √† explorer. Si vous souhaitez approfondir vos connaissances sur la tokenisation et d√©couvrir les diverses approches disponibles, je vous invite √† consulter cette ressource [documentation](https://huggingface.co/learn/nlp-course/chapter1/3?fw=pt).



# vocabulaire 

Le vocabulaire dans le traitement automatique du langage naturel (TALN) est comme un dictionnaire qui associe des mots √† des num√©ros.

Par exemple:
```javascript
Tokens :["je","suis","content"]

vocab: {"je":1,"suis":2,"content":3}

vocab_size=3

```

# Embeddings

Un  embedding  est une reprensentation de chaque element du vocabulaire dans un space de dimensions n generalement on choisit une dimension n=512 pour les Embeddings .

Par exemple avec le vocabulaire construisons un embeddingd de dimensions n=5

```javascript
{
    1: [0.1, -0.2, 0.3, -0.4, 0.5],   # Pour le mot "je"
    2: [-0.6, 0.7, -0.8, 0.9, -1.0],  # Pour le mot "suis"
    3: [0.2, -0.3, 0.4, -0.5, 0.6]    # Pour le mot "content"
}
```

# Encodage de position

Encodage de position est une technique utilis√©e pour indiquer l'ordre des mots dans une s√©quence de tokens. La formule utilis√©e pour cela est la suivantes : 

![Logo](https://miro.medium.com/v2/resize:fit:676/1*NBetROvAUpwf3KH31-nAOg.png)

Avec : 
-  h ∑ : Taille de la dimension de l'int√©gration des mots.
- pos : position du mot courant dans la s√©quence dans [0, N-1]
- i : index de l'index dimensionnel de l'incorporation de mots dans [0, h ∑-1] 


Nous allons calculer  l'encodage de position pour l'exemple precedent 

```javascript
Tokens :["je","suis","content"]

pos=  [ 0   ,   1   ,    2  ]

h ∑= 5 car nous l'avons choisit precedamment pour creer nos integrations 

i= 0 a 4
```

```
pour pos=0 qui correspond au mot "je" nous avons :

PE(0,2*0)=PE(0,0)=sin(0/10000^(2/5)=0
PE(0,2*0+1)=PE(0,1)=cos(0/10000^(2/5)=1
PE(0,2*1)=PE(0,2)=sin(1/10000^(2/5)=0.12
PE(0,2*1+1)=PE(0,3)=cos(1/10000^(2/5)=0,99
PE(0,2*2)=PE(0,4)=sin(2/10000^(2/5)=0,0012

donc l'encodage de position du mot "je" est : [0,1,0.12,0.99,0.0012]

supposons que nous avons pour :

"suis":[0.1,1.1,0.12,0.99,0.001]

"content":[0.01,1.2,0,12,0.9,0.0012]



```

# Mecanisme d'attention


![Logo](/images/t6qJz.png)

Le m√©canisme d'attention  est une technique utiliser  qui permet au mod√®le de comprendre les relations et les d√©pendances entre les mots d'une s√©quence.

Il existe deux types de m√©canismes d‚Äôattention : le m√©canisme d‚Äôattention simple et celui √† t√™tes multiples.

Le m√©canisme d‚Äôattention simple capture la relation entre chaque paire de mots, tandis que l‚Äôattention multi-t√™tes est une extension de l‚Äôattention simple qui permet au mod√®le de capturer diff√©rentes informations .

En d‚Äôautres termes, l‚Äôattention multi-t√™tes permet au mod√®le de se concentrer simultan√©ment sur plusieurs parties d‚Äôun texte, am√©liorant ainsi ses performances.

- ### Matrice Q (Query) 

La matrice Q repr√©sente les repr√©sentations requ√™te des mots dans une s√©quence.

Elle est obtenue en appliquant une transformation lin√©aire aux repr√©sentations d'entr√©e des mots (par exemple, des embeddings de mots).

La taille de la matrice Q est g√©n√©ralement (nombre_de_mots, dimension_de_la_repr√©sentation)

```
Q = W_q * X

```
W_q : Matrice de poids de la requ√™te, g√©n√©ralement de dimension (dimension_de_la_repr√©sentation, dimension_de_la_requ√™te).

X : Matrice d'entr√©e, g√©n√©ralement de dimension (nombre_de_mots, dimension_d'entr√©e).

- ### Matrice k (Key) 

La matrice K repr√©sente les repr√©sentations cl√© des mots dans une s√©quence.

Elle est obtenue en appliquant la m√™me transformation lin√©aire que pour la matrice Q aux repr√©sentations d'entr√©e des mots.

La taille de la matrice K est identique √† celle de la matrice Q.

```
K = W_k * X

```
dim_W_k =dim_W_q

- ### Matrice V (value)



La matrice V repr√©sente les repr√©sentations valeur des mots dans une s√©quence.

Elle est g√©n√©ralement obtenue en appliquant une transformation lin√©aire diff√©rente que pour les matrices Q et K aux repr√©sentations d'entr√©e des mots.

La taille de la matrice V est g√©n√©ralement (nombre_de_mots, dimension_de_la_repr√©sentation).

```
V = W_v * X

```
dim_W_v =dim_W_q


# Propagation en avant
![Logo](https://miro.medium.com/v2/resize:fit:640/format:webp/1*yi_TsViKNs2ned-0RXK6NQ.png)

La couche de propagation en avant dans les Transformers est un composant essentiel qui permet au mod√®le d'apprendre des relations complexes entre les mots et d'am√©liorer ses performances sur diverses t√¢ches de traitement du langage naturel.

Il s'agit d'une transformation par position qui consiste en une transformation lin√©aire, ReLU et une autre transformation lin√©aire.

# Encodeur 
![Logo](images/Detailed-view-of-a-transformer-encoder-block-It-first-passes-the-input-through-an.png)
L'encodeur est responsable de la transformation de la s√©quence d'entr√©e en une repr√©sentation vectorielle. Il est compos√© de plusieurs couches d'encodeur, chacune contenant un m√©canisme d'attention et un feed forward.
Les mod√®les d'encodeur sont les mieux adapt√©s aux t√¢ches n√©cessitant une compr√©hension de la phrase compl√®te, telles que la classification de phrases, la reconnaissance d'entit√©s nomm√©es (et plus g√©n√©ralement la classification de mots) et la r√©ponse extractive √† des questions.

Par exemple 
```
ALBERT
BERTE
DistilBERT
√âLECTRE
Roberta
```

# Decodeur

![Logo](images/de.png)

Les mod√®les de d√©codeur utilisent uniquement le d√©codeur d'un mod√®le de transformateur.

Les mod√®les de d√©codeur utilisent uniquement le d√©codeur d'un mod√®le de transformateur. A chaque √©tape, pour un mot donn√©, les couches d'attention ne peuvent acc√©der qu'aux mots positionn√©s avant lui dans la phrase. Ces mod√®les sont souvent appel√©s mod√®les auto-r√©gressifs .

Ces mod√®les sont les mieux adapt√©s aux t√¢ches impliquant la g√©n√©ration de texte.

Par exemple 
```
CTRL
Google Tag
GPT-2
Transformateur XL
```

# Encodeur-decodeur 

![Logo](images/Detailed-view-of-a-transformer-encoder-block-It-first-passes-the-input-through-an.png)

Les mod√®les codeur-d√©codeur (√©galement appel√©s mod√®les s√©quence √† s√©quence ) utilisent les deux parties de l'architecture Transformer.

A chaque √©tape, les couches d'attention de l'encodeur peuvent acc√©der √† tous les mots de la phrase initiale, alors que les couches d'attention du d√©codeur ne peuvent acc√©der qu'aux mots positionn√©s avant un mot donn√© dans l'entr√©e.

Les mod√®les s√©quence √† s√©quence sont les mieux adapt√©s aux t√¢ches consistant √† g√©n√©rer de nouvelles phrases en fonction d'une entr√©e donn√©e, telles que le r√©sum√©, la traduction ou la r√©ponse g√©n√©rative √† des questions.
Par exemple
```
BART
mBART
Mariale
T5
```
